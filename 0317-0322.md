文章食用说明：本文主要面向对象是自己，所以有些内容非常个人化；且这个文档目前还未整理完成，后续还会更新~
# 前言
春天来了，当人停下来的时候，感受身旁的微风，什么都不想，什么都想，就会忽地发现，时间过得好快好快啊，仿佛长大后时间被无限加速。这时，再来回忆昨天干了什么、这一周干了什么、这一个月干了什么，脑子会先恍惚一下，仔细想想，哦，原来自己也做了一些事情。

这一周的主要目标是了解意图识别的发展历程、研究现状、研究挑战、研究方向和技术路线，总之是一些很综述的东西。

意图识别是对话系统中的一个很重要的研究方向，对话系统又可以分为任务型和开放性对话系统，意图识别均在其中有应用。此外，意图识别还应用在搜索引擎领域。意图识别可以看作是文本分类的特例。

优秀意图识别相关论文列表：[thuiar/OKD-Reading-List： 开放知识发现论文](https://github.com/thuiar/OKD-Reading-List)
工具包：[thuiar/TEXTOIR: TEXTOIR is the first opensource toolkit for text open intent recognition. (ACL 2021)](https://github.com/thuiar/TEXTOIR?tab=readme-ov-file)
# 研究方向
目前我了解到的研究方向有以下三点：
- **意图检测(Intent Detection)**
	- 无监督 / 半监督
	- few-shot / zero-shot
- **新意图发现(New Intent Discovery)**：从少量有标签数据和大量无标签数据中检测已知意图和发现新意图
	- 无监督
	- 半监督 / 自监督
（[2012.08987](https://arxiv.org/pdf/2012.08987)Figure1）
- **开放意图检测(Open Intent Discovery)**：包含前面两个任务
>[!question]
>开放意图检测和NID的区别是什么

# 研究挑战
## 新意图发现
- 如何利用已知意图的先验知识
- 如何更好的对话语进行语义表示
（发展：浅层神经网络捕捉低级语义信息 -> PLM结合上下文捕捉高级语义表示；对比学习优化；多任务预训练；pipeline -> 端到端）
- 如何更好的聚类，构建高质量的聚类信号

- 聚类分配不一致

- 泛化性能：通用领域 -> 垂直领域
- 数据集的分布：均匀 / 长尾 / ……

# 技术路线
- pipeline：编码器提取话语特征表示，传统聚类方法聚类
- 端到端：
# 论文笔记
1. 这部分内容比较长，且主要是论文直接翻译整理得到。
2. 论文主要围绕两个作者，他们的工作都很优秀。
### 01_Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement
1911.08891_zhl [Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement](https://arxiv.org/pdf/1911.08891)
GitHub："D:\86132\AAA研究生数学\CDAC-plus"
**摘要**
- 新意图发现任务
- 端到端的半监督聚类方法：利用有标签数据的先验知识约束指导聚类过程
**创新点（主要贡献）**
- 提出端到端的聚类方法：不需要密集的特征工程，并且对集群的数量不敏感
- 如何利用预先训练的语言模型和有限的标记数据来帮助聚类过程
**引言**
- 一方面，很难估计*新 intent 的确切数量*。另一方面，由于意图的分类法通常由*启发式方法*决定，因此很难获得所需的聚类结果
- （2018）提取特征--分层聚类--预定义的结构化指导聚类结果：*密集的特征工程  / pipeline method*
- 训练数据：少量有标签+大量无标签 / 无标签数据包含已知意图和新意图

**相关工作**
迁移学习 / few shot learning / 无监督聚类 / 基于深度神经网络的聚类 / 约束聚类 / 
**方法**
- step1：预训练模型提取意图特征表示$I_i$（使用K-means对得到的意图特征表示进行初始聚类，得到聚类中心$U_j$，仅用于初始化，后续会动态更新）
- step2：
	- 2.1使用意图表示来计算句子对的相似性矩阵$S_{ij}(I_i.I_j)$；
	- 2.2我们使用相似或不同的标签训练网络，这些标签由标记数据或动态相似性阈值生成。我们将标记数据提供的成对约束视为先验知识边缘，并使用它来指导聚类过程
	- 2.3 计算软分配概率矩阵Q（$I_i,U_j$）
- step3：最后，我们使用辅助目标分布和 Kullback-Leibler di vergence （KLD） 损失来鼓励模型从高置信度分配中学习
**方法重写版**
- step1：预训练模型提取意图特征表示$I_i$（使用K-means对得到的意图特征表示进行初始聚类，得到聚类中心$U_j$，仅用于初始化，后续会动态更新）
- step2:
	- 2.1 使用意图表示来计算句子对的相似性矩阵$S_{ij}(I_i,I_j)$；
	- 2.2 标签数据直接得到R矩阵；无标签数据设定上下限根据相似度选择样本得到R矩阵
	- 2.3 计算损失，更新预训练模型参数
- step3：
	- 3.1 计算辅助目标分布P：先根据聚类中心$U_j$和意图特征表示$I_i$计算软分配概率矩阵；然后根据$Q_{ij}$计算目标分布$P$；之后计算得到$L_{KLD}$
	- 3.2 使用KL散度损失更新模型参数以输出更好的聚类结果
train：输入：data，输出：聚类结果，得到一个训练好的（参数最优）的模型
test: input: 一句话，output：该话语的意图表示
**我的理解**
**问题**
- 启发式算法
- 使用*分层聚类方法*对句子进行分组；使用预定义的结构化输出来指导聚类过程
- KLD loss
### 02_Discovering New Intents with Deep Aligned Clustering
zhl [2012.08987](https://arxiv.org/pdf/2012.08987)
**摘要**
- 大多数现有方法在将先验知识从已知意图转移到新意图方面受到限制。它们在提供高质量的监督信号来学习用于对未标记的意图进行分组的聚类友好特征方面也存在困难。
- 我们提出了一种有效的方法，即深度对齐聚类，以借助有限的已知意图数据来发现新的意图
- 首先，利用一些标记的已知意图样本作为先验知识来预训练模型。然后，我们执行 k-means 以生成作为伪标签的集群分配。此外，我们提出了一种*对齐策略*来解决筛选任务期间的标签不一致问题。最后，我们学习了在对齐的伪标签的监督下进行意图表示。对于未知数量的新意图，我们通过消除低置信度的意图分类来预测意图类别的数量。
**创新点**
我们将我们的贡献总结如下。
- 首先，我们提出了一种简单有效的方法，该方法成功地将新意图生成为大量新意图，并估计对已知意图中已知的先验知识有限的新类别的数量。
- 其次，我们提出了一种有效的对齐策略，通过学习判别性特征来区分已知和新的意图，从而获得高质量的自我监督信号。
- 最后，对两个基准数据集的广泛实验表明
**引言**
- 上述所有方法都*无法利用已知意图的先验知识*。这些方法假定未标记的样本仅由未识别的新意图组成。更常见的情况是，已知 intent 的一些标记数据是可访问的，而未标记的数据与已知 intent 和新 intent 混合在一起
- 我们之前的工作 CDAC+ （Lin， Xu， and Zhang 2020） 直接解决了这个问题。尽管如此，它使用成对相似性作为弱监督信号，这些信号是模棱两可的，以掩盖未标记的已知和新意图的混合。因此，随着更多新意图的出现，性能会下降。
- 两个主要困难
	- 使用有限的标记数据有效地将先验知识从已知意图转移到新意图是具有挑战性的。
	- 很难构建高质量的监督信号，用于学习对未标记的已知和新意图进行聚类的友好表示
- 
**方法**
**我的理解**
### 03_A Clustering Framework for Unsupervised and Semi-supervised New Intent Discovery
zhl_2304_[A Clustering Framework for Unsupervised and Semi-supervised New Intent Discovery](https://arxiv.org/html/2304.07699v3)
**摘要**
- *标记数据的先验知识有限或没有可用*时，大多数现有方法都难以捕获离散文本表示的复杂语义
- 提出USNID：
	- 充分利用无监督或半监督数据来挖掘*浅层语义相似性关系*，并为聚类提供初始化良好的表示
	- 其次，它设计了一个质心引导的聚类机制来解决*聚类分配不一致*的问题，并为表示学习提供高质量的自监督目标。
	- 第三，它捕获无监督或半监督数据中的高级语义，通过优化集群级和实例级目标来发现精细的意图级集群。
- 我们还提出了一种有效的方法，用于在开放世界场景中估计集群数量，而无需事先知道新意图的数量。
- USNID 在多个基准意图数据集上表现异常出色，在无监督和半监督新意图发现方面取得了新的最新结果，并在不同的集群编号下展示了稳健的性能。
**引言**
- 
- Intent Detection：发现新意图 / 标签数据的稀缺性
- new intent discovery task，这是一个集群问题。
	- 对于*半监督式新 intent 发现*，我们随机选择一部分 intent 类为 known，其余部分为 new intents。考虑到实际应用中标记数据的稀缺性，我们用已知意图（即 90%）屏蔽了大多数标签。掩码的已知意图样本和新意图样本构成未标记的数据。目标是使用有限的标记和大量未标记的数据来查找已知和发现新的意向组。
	- 对于*无监督的新 intent 发现*，它的目标是在没有任何标记数据先验知识的情况下发现新的 intent 组。
 - *新品类发现 （NCD)* 假定未标记的数据仅来自新类别，这在*实际场景中不适用*，因为未标记数据通常包含已知类别和新类别的混合。虽然 *GCD*提出了一个通用设置来解决这个问题，它仍然需要更大比例的标记数据（例如，50% 对 10%），并且没有为无监督设置提供解决方案。此外，实验表明，NCD 和 GCD由于*难以学习离散文本表示的复杂语义*，因此在应用于我们的任务时存在局限性。
 - NID有三个主要挑战。
	 - 首先，当前方法仍然严重依赖标记数据，并且在没有任何额外知识的完全无监督环境中，它们的性能会受到严重影响
	 - 其次，在半监督场景中，我们需要充分利用有限的标记数据，并转移其知识来引导未标记的数据学习有利于聚类的意图表示。
	 - 第三，新 intent 的数量可能无法提前知道。在这种情况下，有效估计集群数量也是确定最终性能的关键因素。
**相关工作**
 - 无监督聚类技术：
	 - K-means及其变体
	 - 深度嵌入式聚类 （DEC）采用堆叠式自动编码器 （SAE）用于低维特征学习和集群分配优化
	 - 深度集群网络 （DCN）还使用 SAE 来优化重建损失和类似 K-Means 的正则化。
	 - 深度自适应集群 （DAC从置信样本中学习成对相似性，
	 - 而 DeepCluster在聚类和特征学习之间交替。
	 - 无监督对比学习是一种上升方法，如执行实例和集群级对比学习的对比聚类 （CC） 和 SCCL，它优化了实例级对比损失和聚类损失，以实现卓越的文本聚类。
- 约束聚类：旨在通过额外的监督信号（例如，标记数据）来增强无监督聚类
- NCD（Novel class discovery）:在CV中旨在使用标记数据识别新的视觉类别
- NID

**方法**
- 4.1intent representation
- 4.2无监督训练
	- 4.2.1无监督预训练
		- 我们在预训练模型时的目标是将不同的样本分开，同时捕获增强对之间的隐式语义关系。
		- 构造正样本对，随机掩码
		- 预训练后，我们移除了对比学习中使用的 head，以避免任何可能干扰后续步骤的不必要偏差。其余的 backbone 被保存用于聚类和表示学习
		- 
	- 4.2.2质心引导的聚类
		- 分区聚类方法（如 K-Means 算法）可用于发现意向性聚类。但是，其有效性可能会因初始质心的选择而受到影响。我们的工作使用 K-Means++ 进行集群化。
		- 由于缺乏帮助增强意图表示能力的指导，直接使用 K-Means++ 的性能仍然很差。因此，我们的目标是*使用聚类信息来构建高质量的**自监督信号**，用于学习高级意图表示。实现此目的的一种自然方法是使用集群分配作为监督的伪标签。这是基于*用作弱监督信号的神经网络的结构化输出也可以使无监督表示学习受益*。然而，由于*质心选择随机性*，同一样本可能会在多次迭代中被分配到不同的集群，因此出现了挑战
		- 在这项工作中，我们引入了一种*新的质心导向机制*来解决训练迭代之间自我监督目标的不一致，并增强分类器中的知识保留。
		- *4.2.2总结*：质心选择（k-means++）和高质量自监督信号构建（伪标签），但会导致每次迭代目标不一致（新的质心导向机制）
	- 4.2.3自监督学习
- 4.3半监督训练
	- 
**实验**
**我的理解**
**问题**
- NCD新品类发现 / GCD
- 不同聚类中的相同样本可能不一致
- sentence representation v.s. intent representation
- 分层聚类方法
- SOTA无监督聚类方法，如 CC 和 SCC
### 04_Towards Real-world Scenario: Imbalanced New Intent Discovery
**文章核心内容**
**创新点**
- 提出不平衡新意图发现任务
- 构建三个新的数据集
- 提出ImbaNID: 模型预训练/可靠伪标签生成/鲁棒表示学习
- 实验效果很好
**后续研究方向**
- 时间复杂度问题：最优传输问题建模为全局方法
- 意图类别的语义理解问题：文章只是识别（已知和未知）意图，但是并未（结合LLMs）给出具体的语义解释
**我的理解**
论文指出在意图识别领域从前的研究都假设数据使均匀分布，但是在真实场景中数据多为长尾分布，论文就是基于这个点，提出不平衡新意图发现任务（i-NID task），后续的数据集构建和方法ImbaNID都是针对这一特性提出。我认为论文的方法本质上就是使用深度聚类方法解决长尾分布下的NID问题，各个组件（预训练加微调、最优传输问题建模生成伪标签、正则化和对比聚类优化）都不是新方法。作者的实验很全面，各种消融实验和对比，可视化也很好。但是本文没有提供代码。
**常用技术路线**
没有找到很好的综述类文章。

NID问题可以归结为聚类问题，所以技术路线也都多为聚类方法，从传统聚类方法到现在深度聚类方法，尤其是近两年使用预训练＋对比学习可能是比较常见的路线。
### 01_Effectiveness of Pre-training for Few-shot Intent Classification
[2109.05782](https://arxiv.org/pdf/2109.05782)
**摘要**
- 能在语义非常不同的新领域进行 few-shot 意图分类。IntentBERT 的高效性证实了小样本意图检测的可行性和实用性，其跨不同意图领域的高度泛化能力表明，意图分类任务可能具有相似的底层结构，可以从一小部分标记数据中有效地学习。
- 
**创新点**
**引言**
- 训练一个准确的意图分类器对于开发这种面向任务的对话系统至关重要。但是，一个重要的问题是，当只有*有限数量的标记实例可用*时，如何实现这一点，这在早期开发阶段通常是这种情况。
- induction network / generation-based method / metric learning / self-training
- 虽然在大规模注释数据集上微调预训练语言模型在许多任务（包括意图检测）中产生了显著的改进，但在*新的应用领域*构建大规模注释数据集既*费力又昂贵*。因此，最近的努力致力于通过进行持续的预训练来使预训练的语言模型适应特定任务，例如意图检测。
- 虽然这些方法取得了令人印象深刻的性能，但它们*严重依赖于大规模语料库*的存在
- 在本文中，我们提供了一个肯定的答案标准监督训练，其中包含来自公共数据集的大约 1,000 个标记话语，并获得了一个名为*IntentBERT 的预训练模型*。它可以直接应用于目标域上的小样本意图分类，该*目标域与预训练数据截然不同*，并且性能明显优于现有的预训练模型，而无需对目标数据（标记或未标记）进行进一步微调。
- 此外，IntentBERT 在跨域小样本分类任务上的*高度泛化能力*，由于较大的领域差距和很少的数据限制，通常被认为非常困难，*这表明大多数意图检测任务可能共享一个共同的底层结构，可以从一小部分数据中学习。*
- 联合训练方法，该方案同时优化了源标记数据的分类误差和目标未标记数据上的语言建模损失。这种联合训练方案可以更好地学习语义表示，并明显优于现有的两阶段预训练方法

**方法**
- 监督训练：使用源领域的有标签数据在bert上微调
- 联合训练：利用目标域无标签数据进一步学习特征表示，提高预训练模型的泛化能力
- 少样本：经过预训练，IntentBERT 的参数是固定的，可以立即用作新颖的少数样本意图分类任务的功能提取器。分类器可以是参数化的，例如 Logistic 回归，也可以是非参数的分类器，例如最近邻。参数分类器将使用任务中提供的少数标记示例进行训练，并对 unlabeled 查询进行预测。
**我的理解**
该论文通过使用少量有标签数据并结合无标签数据进行联合训练（提高预训练模型的泛化能力，单阶段学习非两阶段预训练方法）对BERT进行预训练，得到IntentBert，之后使用该模型进行分类预测时表现很好，超越现有的几个预训练基线模型。
**问题**
- 两阶段预训练方法 / few shot / 联合训练
- 公开数据集：
	- 通用，包含多个领域：OOS / HWU64
	- 垂直领域数据集：BANK ING77 / MCID / HINT3
- 分类器是否是参数化？线性层（参数）或直接基于相似度/距离（无参）来分类

#### (1) 两阶段预训练方法（Two-stage Pre-training）
- ​**定义**：先在大规模通用语料（如Wikipedia）上预训练语言模型（第一阶段），再在特定任务相关数据（如对话语料）上继续预训练（第二阶段）。
- ​**论文关联**：基线模型TOD-BERT、CONVBERT均采用此范式，需耗费大量计算资源和数据收集成本。
#### (2) Few-shot（少样本学习）
- ​**定义**：模型在**每个类别仅有极少量标注样本**​（如5个）的场景下进行分类任务。例如，5-way 2-shot表示从5个类别中各取2个样本训练模型。
- ​**论文意义**：IntentBERT通过预学习意图分类的通用表征，使得在目标领域只需极少量样本即可快速适应。
#### (3) 联合训练（Joint Training）
- ​**定义**：同时优化多个目标函数。本文中，模型同时学习**标注数据的分类任务**和**无标签数据的语言建模任务**​（MLM）。
- ​**技术价值**：相比传统的两阶段预训练（先MLM再分类），联合训练避免了领域漂移问题，直接融合语义理解和意图判别能力。
### 02_New Intent Discovery with Pre-training and Contrastive Learning
zyw [[2205.12914] New Intent Discovery with Pre-training and Contrastive Learning](https://arxiv.org/abs/2205.12914)
**摘要**
- 现有方法通常依赖于大量标记的话语，并采用伪标记方法进行表示学习和聚类，这些方法标签密集、效率低下且管理无效。
- （1） 如何学习语义话语表示 （2） 如何更好地对话语进行聚类
- 提出一种*多任务预训练策略*，以利用丰富的未标记数据以及外部有标签数据进行表示学习
- 我们设计了一种*新的对比损失*，以利用未标记数据中的自我监督信号进行聚类。
**创新点**

**引言**
 - *NID的重要性*：为了设计自然语言理解系统，需要事先收集一组预期的客户意图来训练意图识别模型。但是，预定义的 intent 无法完全满足客户需求。这意味着有必要通过反复整合从未标记的用户话语中发现的新意图来扩展意图识别模型。
 - *从前的工作*：为了减少从大量话语中手动识别未知意图的工作量，以前的工作通常采用聚类算法对具有相似意图的话语进行分组。此后的集群分配可以直接用作新的 intent 标签，也可以用作启发式方法以加快注释速度。
 - *研究挑战*：（1） 如何学习语义话语表示 （2） 如何更好地对话语进行聚类
	 - （1）学习语义话语表示为聚类提供适当的提示非常重要。简单地应用一个普通的预训练语言模型来生成话语代表并不是一个可行的解决方案，这会导致 NID 的性能不佳
	 - 最近的一些工作建议使用已知意图的标记进行表示学习，但它们需要大量的已知意图和每个意图的足够标记话语，而这些话语并不总是可用的，尤其是在对话系统的早期开发阶段。
	 - 此外，伪标记方法经常被用来生成用于再现学习和聚类的监督信号
	 - *本文解决方法*：多任务预训练任务 / 与最近邻进行对比学习
 - • 我们表明，我们提出的*多任务预训练方法*已经为无监督和半监督 NID 带来了比最先进的模型更大的性能增益。• 我们通过将*邻居关系纳入对比学习目标*，提出了一种 NID 的*自我监督聚类方法*，这进一步提高了性能。• 我们在三个基准数据集上进行了广泛的*实验和分析研究*，以验证我们方法的有效性。
 **相关工作**
- NID 的研究仍处于早期阶段。开创性的工作集中在 unsupervised 聚类方法上，一些最近的工作研究了用于意图标记的监督聚类算法，但它无法处理新的意图
- 另一系列工作调查了一个更实际的案例，其中提供了一些已知的意图来支持未知意图的发现，这通常被称为半监督 NID
- 为了解决半监督 NID 问题，Lin et al. （2020） 提议首先通过句子相似性任务对已知意图进行监督训练，然后对未标记的话语使用伪标记来学习更好的嵌入空间
- Zhang et al. （2021c） 建议首先对已知的意图进行预训练，然后对未标记的数据进行 k-means 聚类，作为符号伪标签，以便在深度聚类之后进行代表学习（Caron et al.， 2018）。他们还建议对齐集群以加速顶层的学习
- 另一种方法是首先将话语分类为已知和未知，然后用未知话语发现新的意图（Vedula et al.， 2020;Zhang et al.， 2021b）。因此，它依赖于第一阶段的准确分类。
- *意图识别的预训练*：以前的大多数工作都建议以自我监督的方式对开放领域对话进行预训练。最近，一些工作指出，使用相关任务进行预训练可以有效地进行意图识别。
- 
**方法**
- 新意向发现 （NID） 的目的是识别 $D_{unlabeled}$ 中新出现的意向 Cu。NID 可以看作是分布外 （OOD） 检测的直接扩展，我们不仅需要识别 OOD 示例，还需要发现底层集群。
- NID 与零样本学习的另一个不同之处在于，我们在训练过程中不会假设访问任何类型的类信息。
- *多任务预训练任务*：直观地说，分类任务旨在通过外部意图数据集中的注释话语学习意图识别的一般知识，而自我监督任务则使用当前域中收集的话语来学习特定于领域的语义。它们共同支持学习语义话语表示，为后续的聚类任务提供适当的提示。
- *CLNN*
	- 使用内积作为距离度量
	- 方程 2 和传统的对比损失之间的主要区别在于我们如何构建正实例 Ci 集。传统的对比损失可以被视为方程 2 的一个特例，邻域大小为 K = 0，并且同一实例被增加两次以形成正对
	- 对比学习后，可以应用非参数聚类算法（如 k means）来获得聚类分配。
	- 
**我的理解**

**问题**
- 伪标签方法
- 深度聚类方法
- 多任务预训练方法
- 无监督聚类方法
- zero-shot learning
- 半监督NID / 无监督NID

- 论文中数据增强的具体操作
- 对比学习
- 半监督NID和无监督NID的操作区别
- 
**回答**
- Zero-shot：已知“订机票”和“查天气”的意图描述，模型可推断“改签机票”属于“订机票”相关。
- NID：直接对用户对话聚类，发现“改签机票”和“退票”为两个独立的新意图。


**5. 相关工作技术路线**
论文梳理了四条技术路线：
1. ​**无监督聚类方法**： 
    - 早期工作：基于自编码器（SAE）或传统聚类算法（k-means/层次聚类）提取特征。
    - 代表模型：SAE-DEC、SAE-DCN。
2. ​**半监督方法**：
    - 伪标签迭代：如CDAC+用相似度预测任务生成伪标签，DAC通过k-means对齐聚类中心。
    - 已知-未知分离：先分类已知意图，再对剩余数据聚类（如Veduli et al. 2020）。  
3. ​**预训练策略**：
    - 对话语料预训练：如ConveRT在对话数据上继续预训练。
    - 任务相关预训练：如利用NLI数据集学习意图相似性（Zhang et al. 2020）。
4. ​**对比学习应用**：
    - 无监督对比：SimCSE通过Dropout构造正样本。
    - 监督对比：SupCon利用标签信息增强同类样本相似性（如Zhang et al. 2021e）。

### 本周的学习历程
#### 0317 周一
- 不是，感觉文献里来来回回就那么几个人啊？
#### 0318 周二
- 得看看王树森推荐系统
#### 0319 周三
- 发现一个讲数据结构的很好的视频，得抽时间补补（印度小哥）
- 关于shell：CMD，power shell，Linux，git bash
#### 0320 周四
- 李宏毅老师的第三讲竟然早都更新了，我还没看😢
- 一堆待办，收获只有一丢丢
- 晚上只粗看了一篇论文
#### 0321 周五
- 周六再想，已经忘了昨天都干了什么了、
#### 0321 周六
- 上午论文，解决不知道的点，现在也还是不明白
- 中午看了一小会机器学习第三讲，在讲大模型的内部运作机制：注意力机制、川普神经元
- 下午偏微分（跟没看一样），运行代码（出错，纯浪费时间，因为是bash、）
- 晚上总结意图识别（专心不起来）；继续代码（好难弄明白）；大数据分析第七章（还差一点点）

- 其实现在对于意图分类有大概的印象了，但是关于发现新意图，好像还不太清楚，明天得想想这个问题。
- 另外论文其实也就看了4篇，还远远不够，先放弃代码了
#### 0322 周日
- 总结和梳理很重要，计划很重要，需要安排好每个单位段做什么，这样就很好看了，至少心理上是轻松的。
- 上午看了CDAC+，差不多又明白了一点，其他没了、不过有两个结论
	- 新意图发现任务还在起步阶段，这个方向是符合发展趋势的（日渐增多的意图，预设的意图不能满足现实场景和需求）
	- 新意图论文里很多都需要伪标签
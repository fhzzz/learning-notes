## _01_Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization
[2205.07208](https://arxiv.org/pdf/2205.07208)
### 摘要
### 相关工作
### 方法
### 实验
### 我的理解

### 问题
## _02_Deep Unknown Intent Detection with Margin Loss
2019_[Deep Unknown Intent Detection with Margin Loss](https://aclanthology.org/P19-1548.pdf)
### 摘要

### 相关工作
### 方法
### 实验
### 我的理解

### 问题
## _03_Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training
[2106.08616](https://arxiv.org/pdf/2106.08616)
### 摘要

### 相关工作
### 方法
### 实验
### 我的理解

### 问题
## 04_KNN-Contrastive Learning for Out-of-Domain Intent Classification
[2022.acl-long.352.pdf](https://aclanthology.org/2022.acl-long.352.pdf)
### 摘要
- 域外 （OOD） 意图分类是对话系统的一项基本且具有挑战性的任务。以前的方法通常将域内 （IND） 意图特征的区域（在特征空间中）严格要求为紧凑或简单地隐式连接，这假设没有 OOD 意图驻留，以学习判别性语义特征。然后，通常假设 IND 意图特征的分布服从 hy pothetical 分布（主要是高斯分布），并且此分布之外的样本被视为 OOD样本。
- 在本文中，我们从 OOD 意图分类的本质出发，并阐述了它的优化目标。
- 我们进一步提出了一种简单而有效的方法，称为**KNN-对比学习**。我们的方法利用意图中 IND 的 K 最近邻 （KNN） 来学习更有利于 OOD 检测的判别性语义特征。基于密度的新颖性检测算法在我们的方法的本质中是如此有充分的基础，以至于将其用作 OOD 检测算法是合理的，而无需对特征分布提出任何要求。
- 在四个公共数据集上的广泛实验表明，我们的方法不仅可以大幅提高 OOD 检测性能，还可以改进 IND 意图分类，同时对特征分布没有限制。
### 引言
- 区分对话系统的这些话语非常重要，因为识别用户的意图决定了是否可以正确执行后续作。
- 为了解决这个问题，现有的方法根据训练过程中是否使用了广泛的标记 OOD 意图样本，大致可以归纳为两类。
	- **OIDetection**：这些方法可能需要额外的大型且耗时的标记域外样本。此外，人工构造的 OOD 样本无法覆盖实际环境中的所有开放类，因此这种方法有其局限性。
	- **两阶段**：判别语义特征+OOD检测 / 最小化类内方差和最大化类间方差，其动机是通过扩大 IND 和 OOD 意图之间的边距来促进检测，一直被认为是解决这个问题的本质
- **现存问题：OOD intents位置；假设高斯分布的不合理性
	- 以前的方法隐式地假设语义特征的区域是特征空间中的紧凑或简单连接区域，这意味着 OOD 意图仅存在于不同的 IND 类之间，而不存在于 IND 分布中，因此紧密的 IND 语义特征有助于 OOD 检测。然而，OOD 意图在语义空间中的**实际位置不受限制**，它们可以出现在 IND 类之间或 IND 分布中。我们将分布在不同 IND 类之间的 OODintent 称为**inter OOD intents**，将 IND 分布内或可以被由本地 IND 意图样本组成的凸包包围的 OODintent 称为**intra OOD intents**。
	- 同时，我们对 CLINC-FULL 训练集中的 IND 语义特征分布进行高斯假设测试，这是由 （Zeng et al.， 2021a） 学习的。我们发现只有 57% 的 IND 类别符合高斯分布，这说明了之前方法中 OOD 描述的**高斯假设可能不合理**
- 为了解决这些问题，我们使用开放空间风险明确定义了 OOD 意图分类的优化目标（Scheirer et al.， 2012）。与以前只考虑 OOD 内部意图的方法相比，我们提出了一种简单而有效的方法来同时考虑 OOD 内部和内部 OOD 意图。如图 2 （c） 所示，我们利用 IND 意图样本的 k 最近邻作为正样本，并在 MoCo中借助队列获得更多的负样本来**学习判别性语义特征**。我们进一步分析了为什么我们的方法可以更好地降低开放空间风险。直观地说，我们的方法在 OOD 意图周围留下了更多的余量，这可以确保我们采用**基于密度**的基本方法进行 OOD 检测，而**无需对分布做出任何假设。**
- 我们将我们的贡献总结如下。
	- 首先，在开放空间风险之后，我们明确了 OOD 意图分类的优化目标，为解决 OOD 意图分类提供了一个范例。
	- 其次，我们分析了现有方法的局限性，并提出了一种更好的降低经验风险和开放空间风险的新方法。
	- 第三，对四个具有挑战性的数据集进行的广泛实验表明，我们的方法实现了一致的改进，而不受特征分布的限制。
### 相关工作
- **Out-of-domain Detection**
	- 将域外检测作为分类问题，以便通过高维空间中的内核找到超平面或超球体来区分 OOD
	- Scheirer et al. （2012） 首先提出了开放空间风险的定义，并将开放集识别恶意化为约束运算时序问题。
	- 为了获得更好的语义表示，近年来深度神经网络被引入该领域。
		- OpenMax 模型，使用深度网络工作的倒数第二层的分数来区分 OOD。
		- MSP：提出了一个基于 maxi mumsoftmax 概率的基线，以展示网络区分 OOD 和 IND 的能力。
	- 上述方法主要集中在计算机可视化上，并假设（或隐含地）特征区域是紧凑的（简单连接区域）……各种方法……这些方法也限制了特征学习阶段或下游检测阶段的特征分布，无法完全解决域外分类问题。
- **Contrast Learning**
	- 提出了 InfoNCE 损失来测量语义空间中样本对的相似性。
	- 为了获得更多数量的对比学习负样本，He et al. （2020） 引入了动量对比 （MoCo），它构建了一个大型一致的意图字典，促进了对比无监督学习。
	- 随着预训练模型 （PTM） 的普及（Qiu et al.， 2020;Lin等人，2021 年），Dwibedi 等人（2021 年）;Li et al. （2021） 将 PTM 与对比学习范式相结合，采用邻居并使用 MoCo 或 Memory Bank 来获得足够的负样本
### 方法
- **3.1Objective of OOD Intent Classification**
	- **Open space risk**：
	- **Objective of OOD Intent Classification**
		- 经验风险定义：
- **3.2Minimize Empirical Risk**
	- 使用BERT提取意图表示
	- 使用交叉熵损失函数优化经验风险？
	- 
- **3.3KNN-Contrastive Learning**
	- 为了降低在**intra OOD intents**中识别为 IND 的风险，我们不需要将所有属于同一类的 IND 意图样本放在一起，只需将 k 个最近的邻居放在一起，同时将它们从不同的类意图样本中推开
	- 为了实现这个目标，我们通过重写对比损失来得到 KNN 对比损失
- **3.4MomentumContrast is All You Need**
	- 在进行 KNN 对比学习时，我们需要解决两个问题：a） 大批量，我们能选择的样本越多，我们就越有可能找到 k 个最近邻。同时，我们还需要足够的负面因素来区分。b） k-最近邻在训练过程中随着它们的进化而保持一致，否则，KNN 对比训练学习可能不稳定。有趣的是，上面提到的这些问题也是动量对比 （MoCo） （He et al.， 2020） 想要解决的问题。在 MoCo 之后，我们还维护了一个包含 IND 样本的队列，并使用当前批次的特征对其进行更新，同时将最早的特征移出队列。队列将样本大小与批次大小解耦，使我们能够获得更多的负样本（有利于降低开放空间风险）。为了保持一致性，前几批的特征由一个缓慢更新的网络（编码器）编码，其参数是来自查询编码器（另一个网络）的参数的基于动量的平均值，有关详细信息，请参见（He et al.， 2020）。结合 softmax 交叉熵损失和 KNN 对比学习损失，最终微调 obejective 以学习判别特征，如下所示：
	- 
- **3.5Local Outlier Factor**
	- 为了更接近实际场景，我们更喜欢下游检测算法，而不假设 IND 意图的潜在分布。
	- 因此，我们采用了一种简单通用的检测算法 LOF 算法 （Breunig et al.， 2000） 并按照 LOF 分数计算 （Lin and Xu， 2019）
### 实验
### 我的理解
训练阶段：BERT提取特征表示，使用交叉熵损失（公式5）优化经验风险；使用KNN-CL（公式6）更好的区分intra OOD intents，然后两个损失函数结合（公式7）优化更新模型参数，得到一个训练好的模型。这里提出的KNN-CL损失函数更符合论文中提到的实际场景中OOD意图位置不受限制的情况。

测试阶段：使用训练阶段得到的模型提取话语的意图特征表示，不假设IND意图的潜在分布，使用LOF区分已知意图和未知意图，并对已知意图进行分类。这里使用的LOF解决了论文中指出的对IND意图数据假设高斯分布的不合理性的问题。

### 问题
- 开放空间风险
- 


## 05_Learning Discriminative Representations and Decision Boundaries for Open Intent Detection
[IEEE Xplore Full-Text PDF:](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10097558)


## 从聚类的角度分析
- 
### 摘要
## 感悟与总结
### 0401周二
- 下午准备明天的汇报：NID，OID， few-shot。之前对看过的论文很排斥，因为想着已经仔细看过一遍了，不想再整理，现在不知道是习惯了忘记还是怎么，总之能很愉快的回看了。
- 晚上继续，看了一篇开放意图识别的论文
- 李沐对比学习串讲 / MoCo

### 0402周三
- [ ] NID整理
- [ ] 基础知识：对比学习 / 聚类方法

- 上午：整理

### 0403周四
- 大早上就走了，爬山去了，学不了一点
- 累死了
### 0404周五
- 还在爬山
- 晚上回来了，回来讨论英语、洗澡、打电话
### 0405周六
- 做英语PPT
- 洗衣服，K歌，吃饭，确定英语稿子
### 0406周日
- 学不了一点，读了几遍英语稿子，还没背会
- 其实最重要的就是英语pre和进入学习状态，明天下午和晚上都是偏微分，后天偏微分一结束就全是意图识别了，加油
- 今天下午先休整休整，看看意图识别。


- 很难从把侧重点放在聚类重新阅读论文，所以打算周二之前恶补一下聚类知识
- 晚上只看了最基本的k-means,k-means++,后天继续，明天能看多少看多少吧

## 0402汇报
- 情况汇总：
	- 主要看了NID
	- 通读；理清方法到能输出；总结难点和技术路线
	- 基础知识：深度聚类；对比学习；传统NLP
- 小样本
- OID
- NID
